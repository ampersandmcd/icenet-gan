{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages used in this notebook\n",
    "# add importable modules from src to system path for use in this notebook\n",
    "import os\n",
    "import sys\n",
    "src_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "import src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We'll continue by building and training IceNet with our baseline UNet and new GAN architectures in this notebook.\n",
    "\n",
    "Note that most of the magic here happens behind the scenes!\n",
    "\n",
    "See the following files to get a deeper look into the mechanics behind each script call we make in this notebook.\n",
    "- `src/train_icenet.py`\n",
    "- `src/models.py`\n",
    "- `src/metrics.py`\n",
    "- `src/utils.py`\n",
    "\n",
    "In addition to our aforementioned alignment with the [`icenet-paper` repository](https://github.com/tom-andersson/icenet-paper), we note that the PyTorch Lightning training logic is inspired by \n",
    "Joshua Dimasaka, Andrew McDonald, Meghan Plumridge, Jay Torry, and Andrés Camilo Zúñiga González's [`sea-ice-classification` repository](https://github.com/ai4er-cdt/sea-ice-classification)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build and visualise models\n",
    "Our UNet and GAN architectures are defined in `models.py`.\n",
    "\n",
    "But what do they look like? How do we construct them? How do we use them? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validate data flow\n",
    "Before we can train our models, we need to make sure data flows through them properly. \n",
    "\n",
    "Let's do that using the `IceNetDataset` class we set up in the previous notebook to load data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train models\n",
    "Now that we've built our models and verified that data is able to flow through them without any dimension mismatches, let's train them.\n",
    "\n",
    "Training models is a computationally-intensive process amenable to parallelisation and batch submission. \n",
    "\n",
    "As such, we'll forego training any models in this notebook, and instead use command line scripts to train our models.\n",
    "\n",
    "For example, we could train a GAN using the following command in our shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.train_icenet --model=gan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could train a UNet using the following command in our shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.train_icenet --model=unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for parallel training, we can use [SLURM](https://slurm.schedmd.com/documentation.html) to submit a batch job for behind-the-scenes processing as follows.\n",
    "\n",
    "Note that each HPC's SLURM setup is unique, hence you'll likely need to adapt `train_icenet_slurm.sh` for the script to work on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch train_icenet_slurm.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. All set\n",
    "After training our models from the command line and reviewing their logged outputs on [Weights & Biases](https://wandb.ai/), we're ready to move on.\n",
    "\n",
    "We'll continue by generating forecasts with trained models in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
